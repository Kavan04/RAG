# ğŸ“„ RAG PDF Q&A with LangChain + Ollama (LLaMA 3)

This project demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline using [LangChain](https://www.langchain.com/) and [Ollama](https://ollama.ai/) running the **LLaMA 3 model**. It allows you to parse a PDF, embed its contents, and ask questions interactively with the context of the document â€” guided by a customizable **base prompt**.

---

## ğŸš€ Features
- âœ… Load and parse PDF files into structured text chunks  
- âœ… Create embeddings with **FastEmbed** for efficient retrieval  
- âœ… Store and query embeddings using **Chroma vector database**  
- âœ… Use **LangChain Retrieval Chains** to combine retrieval with generation  
- âœ… Run **Ollamaâ€™s LLaMA 3 model** locally for fast and private inference  
- âœ… Provide a **base system prompt** to control responses  

---

## ğŸ› ï¸ Tech Stack
- [Python 3.10+](https://www.python.org/)  
- [LangChain](https://github.com/langchain-ai/langchain)  
- [Ollama](https://ollama.ai) (LLaMA 3 model)  
- [ChromaDB](https://www.trychroma.com/)  
- [FastEmbed](https://github.com/qdrant/fastembed)  
- [PDFPlumber](https://github.com/jsvine/pdfplumber)  

---

## Requirements
- langchain
- langchain-community
- chromadb
- pdfplumber
- fastembed
- ollama
